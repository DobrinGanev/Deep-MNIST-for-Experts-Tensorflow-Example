{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The input images x will consist of a 2d tensor of floating point numbers. \n",
    "#Here we assign it a shape of [None, 784], where 784 is the dimensionality of a single flattened 28 by 28 pixel MNIST image,\n",
    "#and None indicates that the first dimension, corresponding to the batch size, can be of any size. \n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "#The target output classes y_ will also consist of a 2d tensor, \n",
    "# where each row is a one-hot 10-dimensional vector indicating which digit class (zero through nine) the corresponding MNIST image belongs to.\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights W and biases b for our model.\n",
    "\n",
    "# We pass the initial value for each parameter in the call to tf.Variable.\n",
    "# In this case, we initialize both W and b as tensors full of zeros.\n",
    "# W is a 784x10 matrix (because we have 784 input features and 10 outputs)\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "\n",
    "# b is a 10-dimensional vector (because we have 10 classes).\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Before Variables can be used within a session, they must be initialized using that session. \n",
    "# This step takes the initial values (in this case tensors full of zeros) that have already been specified, \n",
    "# and assigns them to each Variable.\n",
    "# This can be done for all Variables at once:\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicted Class and Loss Function\n",
    "\n",
    "# We can now implement our regression model.\n",
    "# It only takes one line! We multiply the vectorized input images x by the weight matrix W, add the bias b.\n",
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can specify a loss function just as easily. \n",
    "# Loss indicates how bad the model's prediction was on a single example; \n",
    "# we try to minimize that while training across all the examples. \n",
    "# Here, our loss function is the cross-entropy between the target and the softmax activation function applied to the model's prediction.\n",
    "# As in the beginners tutorial, we use the stable formulation:\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n",
    "\n",
    "#Note that tf.nn.softmax_cross_entropy_with_logits internally applies the softmax on the model's unnormalized model prediction and sums across all classes, and tf.reduce_mean takes the average over these sums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "# Now that we have defined our model and training loss function, \n",
    "# it is straightforward to train using TensorFlow.\n",
    "# Because TensorFlow knows the entire computation graph, it can use automatic differentiation to find the gradients of the loss with respect to each of the variables. \n",
    "# TensorFlow has a variety of built-in optimization algorithms .\n",
    "# For this example, we will use steepest gradient descent, with a step length of 0.5, to descend the cross entropy.\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The returned operation train_step, when run, will apply the gradient descent updates to the parameters. \n",
    "# Training the model can therefore be accomplished by repeatedly running train_step.\n",
    "for i in range(1000):\n",
    "  batch = mnist.train.next_batch(100)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Evaluate the Model\n",
    "\n",
    "#How well did our model do?\n",
    "\n",
    "# First we'll figure out where we predicted the correct label. tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, tf.argmax(y,1) is the label our model thinks is most likely for each input, while tf.argmax(y_,1) is the true label. We can use tf.equal to check if our prediction matches the truth.\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#That gives us a list of booleans.\n",
    "#To determine what fraction are correct, we cast to floating point numbers and then take the mean. For example, [True, False, True, True] would become [1,0,1,1] which would become 0.75.\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9214\n"
     ]
    }
   ],
   "source": [
    "#Finally, we can evaluate our accuracy on the test data. This should be about 92% correct.\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
